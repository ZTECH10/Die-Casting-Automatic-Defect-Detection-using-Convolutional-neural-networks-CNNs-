{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Die Casting Automatic Defect Detection using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "üè≠ Casting is a manufacturing process in which a liquid material is poured into a mould that contains a hollow cavity of the desired shape and then allowed to solidify. There are three types of casting processes: Sand Casting, Die Casting, and Lost Wax Casting.\n",
    "\n",
    "üõ†Ô∏è There are many types of defects in casting like holes, burr, shrinkage defects, mould material defects, pouring metal defects, metallurgical defects, etc.\n",
    "\n",
    "üè≠ A foundry that produces bearing bushes, in order of 10,000 parts per day has invested large funding to automate the process of finding defects in its casting production line. Currently, the inspection process is carried out manually by QC personnel. It is a very time-consuming process and due to human error, the process of rejecting defects is not very accurate. This can be the cause of rejection of an entire order which would lead to huge losses. The goal is to build a Machine Learning Model to eliminate this loss of revenue and make the QC process as accurate as possible.\n",
    "\n",
    "üìä **Dataset**: All images are (512x512) pixels grey-scaled. There are two Folders, the Training, and the Test set, each containing two subfolders of ‚ÄúDefect‚Äù and ‚ÄúOkay‚Äù parts. ¬†\n",
    "\n",
    "üìù **Task**: To develop a deep-learning classification model for this problem. Evaluate and validate the model accuracy with this Test set. Later, I will  use the model to predict if a single unknown image (some examples in the Folder, New_set) is a defective or okay part. \n",
    "\n",
    "üìÇ **Source of the dataset**: (https://www.kaggle.com/ravirajsinh45/real-life-industrial-dataset-of-casting-product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Pre-processing:**\n",
    "\n",
    "- üß© Split data into training and testing sets\n",
    "\n",
    "- üß©Preprocessing the Training Set:\n",
    "\n",
    "    For the training set, I would apply a series of image transformations to enhance the model's ability to generalize. These transformations would include:\n",
    "\n",
    "    - üîÑ Rescaling: The pixel values in the images are scaled down to a range between 0 and 1, which helps in standardizing the data.\n",
    "    - üìè Shear Range: Images would be subjected to shearing, which involves shifting one part of the image in a fixed direction, creating a sort of 'tilting' effect.\n",
    "    - üîç Zoom Range: Random zooming would be applied to the images to simulate different perspectives.\n",
    "    - ‚ÜîÔ∏è Horizontal Flip:  Some images would be horizontally flipped, which can help the model learn more robust features.\n",
    "\n",
    "- üß©Preprocessing the Test Set:\n",
    "\n",
    "    - üîÑ Rescaling: For the test set, I would only perform rescaling to ensure consistency in the data. This is crucial to ensure that the model sees the data in a format similar to what it was trained on.\n",
    "‚Ä®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7383 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('Casting Dataset/Training_Set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32, # images are put in batches 32 images.\n",
    "                                                 class_mode = 'binary') # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is 231 because 7383/ 32 = ~ 231 . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 863 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255) # we only do scaling for the testing set.\n",
    "test_set = test_datagen.flow_from_directory('Casting Dataset/Test_Set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')# Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Building the CNN Model\n",
    "\n",
    "\n",
    "**Building the CNN Model:**\n",
    "\n",
    "1- Simply, I perform a series convolution + pooling operations, followed by flattening and a number of fully connected layers. Then, I compile and train the model.\n",
    "\n",
    "\n",
    "2- In detail , a CNN model can be thought as a combination of two components: feature extraction part and the classification part. \n",
    "    - The convolution + pooling layers perform feature extraction. For example given an image, the convolution layer detects features such as two eyes, long ears, four legs, a short tail and so on. \n",
    "    - The fully connected layers then act as a classifier on top of these features, and assign a probability for the input image being a dog.\n",
    "\n",
    "3- The convolution + pooling layers as powerhouse:\n",
    "- The convolution layers are the main powerhouse of a CNN model. Automatically detecting meaningful features given only an image and a label is not an easy task.\n",
    "- The convolution layers learn such complex features by building on top of each other. \n",
    "- The first layers detect edges, the next layers combine them to detect shapes, to following layers merge this information to infer that this is a nose. To be clear, the CNN doesn‚Äôt know what a nose is. By seeing a lot of them in images, it learns to detect that as a feature. The fully connected layers learn how to use these features produced by convolutions in order to correctly classify the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Step 1 - Initialising the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "Model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 2 - Adding First Convolution Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 3 - Pooling the First Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a 2x2 pooling window (i.e. pool_size=2) as it‚Äôs the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Step 4 - Adding a Second Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Pooling the Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 6 - Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the output of both convolution and pooling layers are 3D volumes, but a fully connected layer expects a 1D vector of numbers. So I flatten the output of the final pooling layer to a vector and that becomes the input to the fully connected layer. Flattening is simply arranging the 3D volume of numbers into a 1D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 7 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Dense(units=256, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 8 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Step 9 - Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "Model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Training the CNN and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "231/231 [==============================] - 72s 310ms/step - loss: 0.7897 - accuracy: 0.6060 - val_loss: 0.6471 - val_accuracy: 0.6060\n",
      "Epoch 2/50\n",
      "231/231 [==============================] - 47s 203ms/step - loss: 0.5906 - accuracy: 0.6718 - val_loss: 0.5144 - val_accuracy: 0.7323\n",
      "Epoch 3/50\n",
      "231/231 [==============================] - 47s 201ms/step - loss: 0.4246 - accuracy: 0.8025 - val_loss: 0.5865 - val_accuracy: 0.6964\n",
      "Epoch 4/50\n",
      "231/231 [==============================] - 46s 200ms/step - loss: 0.3537 - accuracy: 0.8465 - val_loss: 0.3072 - val_accuracy: 0.8505\n",
      "Epoch 5/50\n",
      "231/231 [==============================] - 46s 200ms/step - loss: 0.2791 - accuracy: 0.8807 - val_loss: 0.9737 - val_accuracy: 0.6570\n",
      "Epoch 6/50\n",
      "231/231 [==============================] - 46s 199ms/step - loss: 0.2441 - accuracy: 0.8994 - val_loss: 0.2213 - val_accuracy: 0.9050\n",
      "Epoch 7/50\n",
      "231/231 [==============================] - 46s 200ms/step - loss: 0.2095 - accuracy: 0.9141 - val_loss: 0.3698 - val_accuracy: 0.8494\n",
      "Epoch 8/50\n",
      "231/231 [==============================] - 47s 201ms/step - loss: 0.1943 - accuracy: 0.9221 - val_loss: 0.3554 - val_accuracy: 0.8586\n",
      "Epoch 9/50\n",
      "231/231 [==============================] - 47s 201ms/step - loss: 0.1738 - accuracy: 0.9323 - val_loss: 0.4479 - val_accuracy: 0.8436\n",
      "Epoch 10/50\n",
      "231/231 [==============================] - 47s 201ms/step - loss: 0.1486 - accuracy: 0.9419 - val_loss: 0.1567 - val_accuracy: 0.9374\n",
      "Epoch 11/50\n",
      "231/231 [==============================] - 46s 200ms/step - loss: 0.1315 - accuracy: 0.9480 - val_loss: 0.1969 - val_accuracy: 0.9316\n",
      "Epoch 12/50\n",
      "231/231 [==============================] - 46s 201ms/step - loss: 0.1131 - accuracy: 0.9596 - val_loss: 0.1828 - val_accuracy: 0.9258\n",
      "Epoch 13/50\n",
      "231/231 [==============================] - 46s 200ms/step - loss: 0.1216 - accuracy: 0.9561 - val_loss: 0.3104 - val_accuracy: 0.8864\n",
      "Epoch 14/50\n",
      "231/231 [==============================] - 47s 202ms/step - loss: 0.1039 - accuracy: 0.9641 - val_loss: 0.1300 - val_accuracy: 0.9525\n",
      "Epoch 15/50\n",
      "231/231 [==============================] - 190s 825ms/step - loss: 0.0854 - accuracy: 0.9686 - val_loss: 0.1678 - val_accuracy: 0.9374\n",
      "Epoch 16/50\n",
      "231/231 [==============================] - 47s 204ms/step - loss: 0.1040 - accuracy: 0.9592 - val_loss: 0.1132 - val_accuracy: 0.9664\n",
      "Epoch 17/50\n",
      "231/231 [==============================] - 46s 201ms/step - loss: 0.0715 - accuracy: 0.9774 - val_loss: 0.0843 - val_accuracy: 0.9687\n",
      "Epoch 18/50\n",
      "231/231 [==============================] - 47s 202ms/step - loss: 0.0825 - accuracy: 0.9720 - val_loss: 0.0740 - val_accuracy: 0.9815\n",
      "Epoch 19/50\n",
      "231/231 [==============================] - 47s 202ms/step - loss: 0.0786 - accuracy: 0.9724 - val_loss: 0.1166 - val_accuracy: 0.9618\n",
      "Epoch 20/50\n",
      "231/231 [==============================] - 47s 202ms/step - loss: 0.0671 - accuracy: 0.9783 - val_loss: 0.1278 - val_accuracy: 0.9594\n",
      "Epoch 21/50\n",
      "231/231 [==============================] - 47s 202ms/step - loss: 0.0729 - accuracy: 0.9749 - val_loss: 0.0842 - val_accuracy: 0.9791\n",
      "Epoch 22/50\n",
      "231/231 [==============================] - 47s 202ms/step - loss: 0.0628 - accuracy: 0.9778 - val_loss: 0.0462 - val_accuracy: 0.9873\n",
      "Epoch 23/50\n",
      "231/231 [==============================] - 47s 203ms/step - loss: 0.0561 - accuracy: 0.9812 - val_loss: 0.1020 - val_accuracy: 0.9676\n",
      "Epoch 24/50\n",
      "231/231 [==============================] - 47s 203ms/step - loss: 0.0555 - accuracy: 0.9810 - val_loss: 0.1150 - val_accuracy: 0.9548\n",
      "Epoch 25/50\n",
      "231/231 [==============================] - 47s 203ms/step - loss: 0.0691 - accuracy: 0.9768 - val_loss: 0.0408 - val_accuracy: 0.9849\n",
      "Epoch 26/50\n",
      "231/231 [==============================] - 47s 203ms/step - loss: 0.0576 - accuracy: 0.9801 - val_loss: 0.0717 - val_accuracy: 0.9791\n",
      "Epoch 27/50\n",
      "231/231 [==============================] - 47s 203ms/step - loss: 0.0457 - accuracy: 0.9851 - val_loss: 0.1310 - val_accuracy: 0.9560\n",
      "Epoch 28/50\n",
      "231/231 [==============================] - 47s 204ms/step - loss: 0.0555 - accuracy: 0.9794 - val_loss: 0.1183 - val_accuracy: 0.9687\n",
      "Epoch 29/50\n",
      "231/231 [==============================] - 1203s 5s/step - loss: 0.0447 - accuracy: 0.9865 - val_loss: 0.1058 - val_accuracy: 0.9676\n",
      "Epoch 30/50\n",
      "231/231 [==============================] - 47s 205ms/step - loss: 0.0667 - accuracy: 0.9772 - val_loss: 0.1319 - val_accuracy: 0.9583\n",
      "Epoch 31/50\n",
      "231/231 [==============================] - 47s 203ms/step - loss: 0.0368 - accuracy: 0.9894 - val_loss: 0.0527 - val_accuracy: 0.9803\n",
      "Epoch 32/50\n",
      "231/231 [==============================] - 47s 203ms/step - loss: 0.0384 - accuracy: 0.9878 - val_loss: 0.0674 - val_accuracy: 0.9768\n",
      "Epoch 33/50\n",
      "231/231 [==============================] - 47s 204ms/step - loss: 0.0416 - accuracy: 0.9875 - val_loss: 0.0591 - val_accuracy: 0.9803\n",
      "Epoch 34/50\n",
      "231/231 [==============================] - 47s 202ms/step - loss: 0.0483 - accuracy: 0.9837 - val_loss: 0.0441 - val_accuracy: 0.9861\n",
      "Epoch 35/50\n",
      "231/231 [==============================] - 47s 204ms/step - loss: 0.0808 - accuracy: 0.9713 - val_loss: 0.2316 - val_accuracy: 0.9351\n",
      "Epoch 36/50\n",
      "231/231 [==============================] - 47s 203ms/step - loss: 0.0444 - accuracy: 0.9843 - val_loss: 0.0784 - val_accuracy: 0.9780\n",
      "Epoch 37/50\n",
      "231/231 [==============================] - 46s 198ms/step - loss: 0.0432 - accuracy: 0.9870 - val_loss: 0.1276 - val_accuracy: 0.9676\n",
      "Epoch 38/50\n",
      "231/231 [==============================] - 46s 200ms/step - loss: 0.0418 - accuracy: 0.9867 - val_loss: 0.0892 - val_accuracy: 0.9745\n",
      "Epoch 39/50\n",
      "231/231 [==============================] - 46s 199ms/step - loss: 0.0399 - accuracy: 0.9858 - val_loss: 0.0828 - val_accuracy: 0.9803\n",
      "Epoch 40/50\n",
      "231/231 [==============================] - 46s 200ms/step - loss: 0.0468 - accuracy: 0.9855 - val_loss: 0.0946 - val_accuracy: 0.9710\n",
      "Epoch 41/50\n",
      "231/231 [==============================] - 46s 199ms/step - loss: 0.0325 - accuracy: 0.9893 - val_loss: 0.0406 - val_accuracy: 0.9873\n",
      "Epoch 42/50\n",
      "231/231 [==============================] - 46s 199ms/step - loss: 0.0277 - accuracy: 0.9913 - val_loss: 0.0557 - val_accuracy: 0.9884\n",
      "Epoch 43/50\n",
      "231/231 [==============================] - 1200s 5s/step - loss: 0.0295 - accuracy: 0.9905 - val_loss: 0.0688 - val_accuracy: 0.9791\n",
      "Epoch 44/50\n",
      "231/231 [==============================] - 50s 217ms/step - loss: 0.0260 - accuracy: 0.9919 - val_loss: 0.0675 - val_accuracy: 0.9768\n",
      "Epoch 45/50\n",
      "231/231 [==============================] - 53s 229ms/step - loss: 0.0279 - accuracy: 0.9915 - val_loss: 0.4633 - val_accuracy: 0.9119\n",
      "Epoch 46/50\n",
      "231/231 [==============================] - 49s 210ms/step - loss: 0.0467 - accuracy: 0.9836 - val_loss: 0.0484 - val_accuracy: 0.9861\n",
      "Epoch 47/50\n",
      "231/231 [==============================] - 48s 206ms/step - loss: 0.0322 - accuracy: 0.9896 - val_loss: 0.0551 - val_accuracy: 0.9826\n",
      "Epoch 48/50\n",
      "231/231 [==============================] - 48s 207ms/step - loss: 0.0230 - accuracy: 0.9932 - val_loss: 0.0156 - val_accuracy: 0.9954\n",
      "Epoch 49/50\n",
      "231/231 [==============================] - 50s 216ms/step - loss: 0.0222 - accuracy: 0.9932 - val_loss: 0.0668 - val_accuracy: 0.9815\n",
      "Epoch 50/50\n",
      "231/231 [==============================] - 48s 206ms/step - loss: 0.0313 - accuracy: 0.9893 - val_loss: 0.0658 - val_accuracy: 0.9826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16f15d580>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.fit(x = training_set, validation_data = test_set, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: val_accuracy is really the accracy of the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Defect': 0, 'Ok': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part is Defective!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('Casting Dataset/InLineProduction_Detection/Part10.jpeg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = Model.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'Part is Okay.'\n",
    "else:\n",
    "  prediction = 'Part is Defective!'\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
